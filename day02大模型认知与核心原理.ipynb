{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc95cbc1-0a31-45b9-b947-fe1b0f34098d",
   "metadata": {},
   "source": [
    "# day02大模型认知与核心原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc273b-48f3-4b98-ac63-f67f3ee2ef02",
   "metadata": {},
   "source": [
    "## day02上课内容"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a29e020-6dff-4bbd-b69d-78b1075af973",
   "metadata": {},
   "source": [
    "内容小标题:\n",
    "    1、什么是AI\n",
    "    2、AI1.0到2.0的变迁\n",
    "    3、大模型与通用人工智能\n",
    "    4、GPT模型的发展历程\n",
    "    5、国产大模型介绍\n",
    "    6、大模型的趋势和挑战\n",
    "    7、大模型赋能行业分析\n",
    "    8、大模型核心原理\n",
    "\n",
    "通过本节课你能够学到什么?\n",
    "    1、能够对传统人工智能「AI」与生成式人工智能「AIGC」建立相关性的认知\n",
    "    2、真正进入大语言模型的学习\n",
    "    3、开始逐渐了解大语言模型核心原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44d3b3-095d-4604-8d2a-1d977d6972cc",
   "metadata": {},
   "source": [
    "## 什么是AI(Artificial Intelligence)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5124ef-130c-4c4a-87c3-bfcec1dcbcd3",
   "metadata": {},
   "source": [
    "<img src=\"./img/AI.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6892af57-eb42-406c-bfb6-d9f9485f0046",
   "metadata": {},
   "source": [
    "将人的思想量化形成可执行的策略或者从本质上模仿人脑的结构仿生。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d805600-0e1f-4aae-95da-91c4cb5ae9bf",
   "metadata": {},
   "source": [
    "## AI(Artificial Intelligence)的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cb9789-3c60-4770-9151-7bd0604fe821",
   "metadata": {},
   "source": [
    "<img src=\"./img/AI的分类.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6affa9f-5281-4cbc-8f3a-db73c555eea4",
   "metadata": {},
   "source": [
    "## AI1.0到2.0的变迁"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447fc2f-d657-4ee2-8eaf-3ea650c5ec86",
   "metadata": {},
   "source": [
    "### AI1.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39255648-0035-4089-ab8d-b977c1696ae0",
   "metadata": {},
   "source": [
    "定义:通常指的是人工智能的早期阶段，这个阶段的人工智能主要是基于规则的系统和早期的机器学习算法。\n",
    "代表产品:特定任务专家"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20f928-0b29-416d-bb64-41ed6cc090ec",
   "metadata": {},
   "source": [
    "### AI2.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41762423-452e-424b-97bd-e992c7d07332",
   "metadata": {},
   "source": [
    "定义:则指的是人工智能技术的现代阶段，这个阶段以深度学习、大数据和云计算的兴起为标志。\n",
    "代表产品:ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf94cdc-7dcc-4b17-afb7-e415a68b7952",
   "metadata": {},
   "source": [
    "参考文章:\n",
    "https://baijiahao.baidu.com/s?id=1760330663034510021&wfr=spider&for=pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730bd61-1232-4efe-b1bf-a100fd54f560",
   "metadata": {},
   "source": [
    "## 大模型与通用人工智能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1c772-4c05-4330-a3be-42e84a4dc7a6",
   "metadata": {},
   "source": [
    "###  大模型的定义"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c187fd6-263f-4277-986d-14ac9a75b285",
   "metadata": {},
   "source": [
    "    大模型，全称「大语言模型」，英文「Large Language Model」，缩写「LLM」。是一种基于机器学习和自然语言处理技术的模型，它通过对大量的文本数据进行训练，来学习服务人类语言理解和生成的能力。 \n",
    "下图所示为常见的大模型对话产品:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0c3ba-8d2e-49dc-8c10-1c5c75ded570",
   "metadata": {},
   "source": [
    "<img src=\"./img/大模型对话产品.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9dfb0bbd-eb47-458f-af3d-fcfda6ddbae6",
   "metadata": {},
   "source": [
    "注意点: 对话产品和基座大模型实际上是两个东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa63c0-1541-4bad-b0ab-e0f4fb9a0347",
   "metadata": {},
   "source": [
    "### 通用人工智能的定义"
   ]
  },
  {
   "cell_type": "raw",
   "id": "945cc09b-ca90-4950-aac7-b5466b29e7b5",
   "metadata": {},
   "source": [
    "    简称AGI，指的是一种智能，能够理解、学习和应用知识和技能，在任何人类智能能够执行的广泛任务上表现得和人类一样好，甚至更好。AGI是一个未来的目标，目前尚未实现，它需要能够处理极其广泛的问题和环境，具有很高的适应性、自主性和创造性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabe702-2f99-44e9-ba2b-ed6ee97afab2",
   "metadata": {},
   "source": [
    "### 大模型与通用人工智能的联系"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2b822-b1a2-4e53-a5bc-4c69b5c4fb19",
   "metadata": {},
   "source": [
    "+ 研究基础：当前的大模型是通向AGI的一种可能的研究途径。通过开发和训练大模型，研究者可以探索智能行为的各种方面，包括语言理解、问题解决和学习能力。例如，通过改进算法、增加模型的泛化能力，以及探索更有效的学习方法，大模型可以逐步接近AGI的特性。\n",
    "+ 技术搭桥：大模型在处理复杂任务时展示的能力可能为发展通用人工智能提供技术基础。\n",
    "+ 实验平台：大模型提供了一个实验平台，研究者可以在这些平台上测试不同的理论和方法，看它们在实际应用中的表现如何，这对于理解和创建AGI至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b3e70-b8d0-4bfd-a823-2744d6ad7ae2",
   "metadata": {},
   "source": [
    "## GPT模型的发展历程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9070cf9-1b53-47d6-9dc2-626e3f3172da",
   "metadata": {},
   "source": [
    "GPT-1\n",
    "+ 发布时间: 2018年\n",
    "+ 模型概述: GPT-1是由OpenAI开发的首个GPT模型，基于Transformer架构。它标志着使用大规模预训练模型在自然语言处理领域的一个重要转变。该模型通过无监督学习从大量文本中预训练语言模型，然后通过有监督学习进行特定任务的微调。\n",
    "+ 关键特点: 该模型展示了通过预训练和微调相结合的方法，可以在多个自然语言理解任务上实现显著的性能提升。\n",
    "  \n",
    "GPT-2\n",
    "+ 发布时间: 2019年\n",
    "+ 模型概述: GPT-2在GPT-1的基础上显著扩展了模型大小和训练数据。具体来说，GPT-2使用了1.5亿个模型参数，远多于GPT-1的参数数量。\n",
    "+ 关键特点: GPT-2显示了出色的语言生成能力，能够产生连贯和引人入胜的文本段落。此外，OpenAI最初由于担心潜在的滥用风险，选择了不完全开放模型的访问。\n",
    "  \n",
    "GPT-3\n",
    "+ 发布时间: 2020年\n",
    "+ 模型概述: GPT-3是一个进一步扩大的模型，拥有1750亿个参数。这一巨大的扩展使GPT-3成为当时最大的语言模型之一。\n",
    "+ 关键特点: GPT-3的性能在多个自然语言处理任务上表现出色，包括翻译、问答和摘要等。GPT-3特别引入了“few-shot learning”，即模型能够在极少量的示例指导下快速适应新任务。\n",
    "  \n",
    "GPT-4\n",
    "+ 发布时间: 2023年\n",
    "+ 模型概述: GPT-4进一步增强了模型的复杂性和多样性，包括改进的训练技术和更广泛的数据集。\n",
    "+ 关键特点: GPT-4在处理复杂的文本理解和生成任务时表现得更加精准，同时在逻辑推理和维持上下文连贯性方面也有显著改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e2b6e-06b0-4c4f-b0d1-156b5a068a05",
   "metadata": {},
   "source": [
    "## 国产大模型介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d8f95-8b80-4cf8-b955-4cd000ca9870",
   "metadata": {},
   "source": [
    "<img src=\"./img/国产大模型.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd5738-4777-4719-939e-03414080d75b",
   "metadata": {},
   "source": [
    "## 大模型的趋势和挑战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad6393-f426-4d56-b78a-d5ea6d1dbe14",
   "metadata": {},
   "source": [
    "发展趋势\n",
    "+ 模型规模的进一步扩大：随着硬件技术的进步和训练技术的改进，预计大模型的规模将继续增长。更大的模型可能带来更强的计算能力和更好的任务泛化能力。\n",
    "  \n",
    "+ 模型效率的提升：大模型的能效和计算效率是未来的重要发展方向。通过算法优化、更高效的架构设计和更先进的硬件支持，模型将在消耗更少资源的同时提供更快的响应和更高的性能。\n",
    "  \n",
    "+ 定制化和专用化：预计将出现更多针对特定领域或特定任务优化的模型，如GPTs系列。这些模型将提供更精准的服务，满足特定行业的需求。\n",
    "  \n",
    "+ 多模态能力的增强：将文本、图像、音频等多种数据模态整合的多模态模型将是未来的一个重要发展方向，使模型能够更全面地理解和生成跨模态内容。\n",
    "  \n",
    "+ 可解释性和透明性的提升：随着模型应用的扩展，用户和监管机构对模型的可解释性和透明性的要求也在增加。未来的模型将需要提供更好的理解和解释其决策过程的能力。\n",
    "\n",
    "挑战\n",
    "+ 伦理和安全问题：随着模型能力的增强，如何确保它们的安全使用和防止滥用成为重大挑战。这包括数据隐私、偏见的减少和滥用风险的控制。\n",
    "\n",
    "+ 资源消耗和环境影响：大模型的训练和部署需要大量计算资源，这带来了显著的能源消耗和环境影响。如何降低这些模型的碳足迹是未来发展的重要考量。\n",
    "\n",
    "+ 数据和模型的治理：随着模型应用的广泛化，如何有效管理使用的数据、保护个人隐私、确保数据安全和合规是另一个挑战。\n",
    "\n",
    "+ 技术普及的不均衡：大模型技术的高成本可能导致技术普及的不均衡，使得资源丰富的机构和国家能够获得更多的好处，而资源较少的地区则可能落后。\n",
    "\n",
    "+ 人机协作的优化：随着AI能力的提升，如何设计机制使人与机器之间的协作更加高效和和谐，特别是在复杂和创造性的任务中，是一个持续的挑战。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050db58-d77a-424d-ad0f-0b33d6cf9bd2",
   "metadata": {},
   "source": [
    "## 大模型赋能行业分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75120b94-3033-4bf5-aac6-0ab13d05cc42",
   "metadata": {},
   "source": [
    "<img src=\"./img/生成式AI在重点行业主流场景的渗透率预测.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb5171-d452-4368-8457-c3d1b1d0731e",
   "metadata": {},
   "source": [
    "<img src=\"./img/AI大模型分类及典型案例.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbb710-1591-4a29-9be2-ba66e38e13e6",
   "metadata": {},
   "source": [
    "## 大模型核心原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f284b-8397-4227-a68d-a16ded0ce745",
   "metadata": {},
   "source": [
    "### 理解大模型成功的背后"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf9ad2-bf6f-441e-80ed-91bbf652c958",
   "metadata": {},
   "source": [
    "1、算法的创新:\n",
    "Transformer架构: 大型语言模型的成功在很大程度上归功于Transformer架构的引入。这种架构通过自注意力机制(self-attention mechanism)能够有效地处理长距离依赖问题，使得模型能够更好地理解文本中的上下文关系。\n",
    "\n",
    "2、预训练与微调: \n",
    "大型语言模型通常采用预训练(pre-training)与微调(fine-tuning)的两阶段学习方法。预训练阶段在大规模语料库上进行，使得模型能够学习到广泛的语言知识；微调阶段则在特定任务上进行，使得模型能够针对特定任务进行优化。\n",
    "\n",
    "3、多任务学习:\n",
    "大型语言模型通常在多个任务上进行训练，这使得模型能够学习到更通用的语言表示，从而在多个任务上都能够取得较好的性能。\n",
    "\n",
    "4、计算能力的提升:\n",
    "GPU与TPU: 大型语言模型的训练需要大量的计算资源，而GPU和TPU等高性能计算设备的出现和普及使得训练大规模模型成为可能。\n",
    "分布式训练: 随着模型规模的不断增大，单机的计算资源已经无法满足需求，因此分布式训练成为了必要的手段。分布式训练通过将计算任务分散到多个计算节点上，大大提高了训练的效率。\n",
    "\n",
    "5、数据的可用性与规模性:\n",
    "大规模语料库: 大型语言模型的训练需要大量的文本数据，而互联网上海量的文本数据为模型的训练提供了丰富的资源。\n",
    "数据质量: 数据的质量对于模型的性能至关重要。因此，在训练模型之前，需要对数据进行清理、预处理等操作，以提高数据的质量。\n",
    "\n",
    "6、软件与工具的进步: \n",
    "深度学习框架: TensorFlow、PyTorch等深度学习框架的出现和普及，为大型语言模型的研发提供了便利。这些框架提供了丰富的API和工具，使得模型的构建、训练和部署变得更加容易。\n",
    "\n",
    "7、开源社区: \n",
    "开源社区的发展为大型语言模型的研发提供了丰富的资源和协作平台。许多研究者和开发者通过开源社区共享自己的研究成果和代码，推动了大型语言模型技术的快速发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe811eae-dabc-4086-96c1-e7899d4b6805",
   "metadata": {},
   "source": [
    "### Transformer架构深度解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc51a9-9740-4c6b-86a8-684f5945bac3",
   "metadata": {},
   "source": [
    "#### 为什么会用到Transformer？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d33f0-234b-4e2e-a6e6-505fd8ad40a4",
   "metadata": {},
   "source": [
    "<mark>实际上是为了解决传统AI领域的一些痛点问题</mark>\n",
    "\n",
    "Transformer出现以前，NLP领域应用基本都是以RNN或LSTM循环处理完成，\n",
    "一个token一个token输入到模型中。模型本身是一种顺序结构，\n",
    "包含token在序列中的位置信息。但是存在了一些问题：\n",
    "\n",
    "    1、会出现梯度消失现象，无法支持长时间序列。越靠后的token对结果的影响越大。\n",
    "    2、只能利用上文信息，无法获取下文信息。 \n",
    "    3、循环网络逐个token输入，也就是句子有多长就要循环多少遍，计算的效率低。\n",
    "    \n",
    "而Transformer的出现得以解决了上述的一系列问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569c56b-3364-47e7-8a06-3566aefdfdb7",
   "metadata": {},
   "source": [
    "+ 大模型中最基本的处理单元 — Token\n",
    "  \n",
    "一条语句由诸多单词（Word）所组成，大模型在进行处理前需要先将语句拆解成一个个的基础单元，但这个基础单元并非单词，而被称为“Token”，一个 Token 大概对应 0.75 个单词。为了简化理解，我们姑且把一个单词就看成一个 Token。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5dcaf3-464d-4209-afab-e0d236025e8f",
   "metadata": {},
   "source": [
    "tokenizer计算: https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23fcacf-ff75-42e2-adfc-f7af753c4264",
   "metadata": {},
   "source": [
    "#### 什么是Transformer？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32991836-94b8-4718-9c42-a1a8ab0d4881",
   "metadata": {},
   "source": [
    "Transformer是一种深度学习模型架构，最初由Vaswani等人在2017年提出。它是一种基于注意力机制的模型，用于处理序列到序列的任务，如机器翻译、语言建模等。Transformer模型的核心思想是完全<mark>基于自注意力机制</mark>（self-attention mechanism），它能够在输入序列中捕捉各个位置之间的依赖关系，从而在处理长序列时具有更好的性能。Transformer通过堆叠多个编码器和解码器层来构建整个模型，每个层都包含多头注意力机制和前馈神经网络。Transformer架构的出现对于自然语言处理领域有着革命性的影响，成为了许多任务的标准模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7f009-afb8-4483-8584-0193387d66c9",
   "metadata": {},
   "source": [
    "#### Transformer架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba05da-f4d7-4285-b053-9b5f3ccb5c1f",
   "metadata": {},
   "source": [
    "##### 宏观层面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b32f5f-655b-4a86-8596-5ba39bdc3e83",
   "metadata": {},
   "source": [
    "首先将Transformer可以看成是一个黑箱操作的序列到序列（seq2seq）模型，输入是单词/字母/图像特征序列，输出是另外一个序列。一个训练好的Transformer模型如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f955db-a821-4382-b43a-80f148f08058",
   "metadata": {},
   "source": [
    "<img src='./img/宏观.gif' width = '100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b27f3a-0318-468f-a812-a849a4a12466",
   "metadata": {},
   "source": [
    "在机器翻译中，就是输入一种语言(一连串单词)，经Transformer输出另一种语言(一连串单词)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3213a-f01b-445d-b923-ada7dbbd0612",
   "metadata": {},
   "source": [
    "<img src = './img/机器翻译.gif' width='100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5990d75-3d72-4931-bc8b-d53274f14c57",
   "metadata": {},
   "source": [
    "拆开这个黑箱，可以看到模型本质就是一个Encoder-Decoder结构，由编码组件、解码组件和它们之间的连接组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba4c92-a224-4ce9-80e0-4d0d29f9015b",
   "metadata": {},
   "source": [
    "<img src = './img/模型本质.png' width='60%'  >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da30b05-8d38-49a4-b530-537a2467066b",
   "metadata": {},
   "source": [
    "每个Encoders中分别由6层Encoder组成，而每个Decoders中同样也是由6层Decoder组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5bf48-ca9f-4393-b9f3-e8e538b12aa2",
   "metadata": {},
   "source": [
    "<img src = './img/模型本质细化.png' width='60%' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e5b83-d046-409b-9667-0bdf9881d9cb",
   "metadata": {},
   "source": [
    "进一步展开ENCODER与DECODER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a7881-8385-4b2a-bac8-00eb0b6722c1",
   "metadata": {},
   "source": [
    "<img src = './img/Encoder与Decoder结构.png' width='60%' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b6218-45e8-4631-88e6-da52571f4f8d",
   "metadata": {},
   "source": [
    "Transfoemer全貌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1c9720-f262-4e08-8154-5ccfa96a71e5",
   "metadata": {},
   "source": [
    "<img src = './img/Transformer全貌架构英文.webp' width='60%' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba72562-9cac-4745-ba1c-a605efffb2a5",
   "metadata": {},
   "source": [
    "<img src = './img/Transformer全貌架构中文.webp' width='60%' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd31237a-2870-4a4d-9437-8e15b8824943",
   "metadata": {},
   "source": [
    "参考:https://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ==&mid=2649887383&idx=3&sn=bbbab92f5927fddb214684216146c7c5&chksm=bf416248c20eafb483bd7071a440acc68d6433f6e0851f906f4de702509019ddc4680cb42821&scene=27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2ba7d-a836-4f4d-a041-c76b173cb13e",
   "metadata": {},
   "source": [
    "### 运行基本机制(重点理解)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a41d0-8402-4855-9760-2fd5983f2646",
   "metadata": {},
   "source": [
    "1）大模型如何理解和表示单词\n",
    "\n",
    "2）大模型如何理解并预测输入的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb62b7-bde2-415e-8c23-4b06b49c4d4c",
   "metadata": {},
   "source": [
    "### 大模型如何理解和表示单词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea4ede-da99-4259-8676-c2c69a9f1f90",
   "metadata": {},
   "source": [
    "##### 大模型中最基本的处理单元 — Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0d781-b931-4a2d-9771-d66ae6ed7e0b",
   "metadata": {},
   "source": [
    "若想让大模型具备理解和创造内容的能力，首先需要让它懂得文本的最基本单位 —— 单词\n",
    "\n",
    "一条语句由诸多单词（Word）所组成，大模型在进行处理前需要先将语句拆解成一个个的基础单元，<mark>但这个基础单元并非单词，而被称为“Token”</mark>，一个 Token 大概对应 0.75 个单词。\n",
    "为了简化理解，我们姑且把一个单词就看成一个 Token。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e1418-3bde-4c29-8fc5-e2446717d089",
   "metadata": {},
   "source": [
    "##### 词与词之间存在不同的远近亲疏关系"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf5b6d-0a5d-46a4-89d2-ab6d3bbe10a6",
   "metadata": {},
   "source": [
    "人类通过学习和阅读掌握词语的含义，大模型同样是通过“阅读”大量的训练语料来学习和理解每个单词该如何使用。\n",
    "\n",
    "大模型逐条“阅读”所有的训练语料后，就能针对每一个单词建立一个<mark>“信息库”</mark>，知道哪些单词“彼此相熟”，哪些则“形同陌路”，\n",
    "\n",
    "模型在学习和理解一个单词的含义时有一个非常重要的视角，<mark>就是看看它周围常常出现的单词都有哪些。</mark>\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "<mark>单词之前的“远近亲疏”关系，是自然语言处理过程中非常重要的信息。</mark>\n",
    "\n",
    "这并不难理解，人类的语言虽然看起来非常灵活，但这种关系也存在一定的统计规律：\n",
    "\n",
    "有些字词的使用频率非常高，\n",
    "中文如“的/是/不/能/知道/可以”，英文如“the/to/and/of/in/for/”。\n",
    "\n",
    "某些个字词经常一同出现从而具有特定的意义，如词组、成语、短语、谚语、俗语。\n",
    "\n",
    "不过，字词之间的远近亲疏并不是固定的，一些场景下会显得“亲密”，一些场景下则变得“疏远”。\n",
    "\n",
    "换句话说，同一个字词，当它周围出现的“小伙伴”不同的时候，可能就会具备不同的含义。如“鲜花”和“花钱”中的“花”，“朝廷”和“朝阳”中的“朝”，再比如“interest”同时有“兴趣”和“利息”的意思。<mark>这里说的场景就是“上下文（Context）”。</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e1122-4e43-4ed9-bfae-f37045ae6ff3",
   "metadata": {},
   "source": [
    "<img src=\"./img/lm-autoregressive.gif\" style=\"margin-left: 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c810b24-0a22-4a78-ba1d-efdb6f71ab22",
   "metadata": {},
   "source": [
    "##### 大模型词义的载体和表现特征：词向量和词嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7973bac-8920-45b1-9a18-713a2458dfb7",
   "metadata": {},
   "source": [
    "这种字词间的远近亲疏关系，在大模型是如何进行表示的呢？这涉及两个非常重要的概念：\n",
    "\n",
    "<mark>词向量（Word Vector）和词嵌入（Word Embedding）。</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc98450-0dfe-41be-ab88-c23dfd439841",
   "metadata": {},
   "source": [
    "从语言学角度，一个词可以从多个维度进行表示，比如：\n",
    "\n",
    "+ 音系维度，即词的发音，如音素、音调、音重等\n",
    "+ 形态维度，即词的形式，如词根、词缀、词形等\n",
    "+ 语义维度，即词的含义，如概念、意义、情感等\n",
    "+ 语法维度，即词的语法结构，如主语、谓语、宾语等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b00359-a7a6-4915-8dd3-d78c422ac67e",
   "metadata": {},
   "source": [
    "词向量可以有多种表现形式，具体会因为采用不同的词向量模型、训练数据集和算法而有所不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c77d54-b470-4c05-a05b-af6eeeae50ce",
   "metadata": {},
   "source": [
    "<mark>在自然语言处理中广泛使用的是词嵌入向量（Word Embedding），简称词嵌入。</mark>学术点的解释是“**通过将离散空间向连续空间映射后得到的词向量**”。每个单词都可以被映射成一个向量，**在映射的过程中，词嵌入向量可以捕捉和记录单词的语义信息，使得语义上相近的单词在向量空间中的距离也相近。** 这种方法能够帮助模型更好的理解和处理自然语言数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142cf67e-8665-4673-b2d7-951d3a069c4b",
   "metadata": {},
   "source": [
    "<img src=\"./img/字转换成词向量.png\" style=\"margin-left: 0px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda331-e66c-4826-abef-5470d22e7cf0",
   "metadata": {},
   "source": [
    "<mark>可以说，词嵌入向量就是通过某种“最佳实践算法”得到的一种词向量。</mark>Word2Vec 是很多大模型都在使用的一种词嵌入向量，它由 Google 在 2013 年开源的 Word2Vec 就是一种流行的词嵌入向量，同时包含了生成词向量相关的工程工具。它利用大规模的语料库进行训练，学习单词之间的语义关系，然后生成每个单词的词向量，然后被用于文本分类、情感分析、机器翻译等各种自然语言处理任务。除了 Word2Vec，FastText、n-gram、GloVe 等也都是常见的词嵌入模型，它们的核心原理是类似的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f296b-2b1c-4b5b-8773-8fb892b3dd30",
   "metadata": {},
   "source": [
    "**注意点:** <mark>词嵌入向量并不直接表示语义，而是词与词之间语义的相似度。因此，不必去纠结每个向量值到底代表什么意思。</mark>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
